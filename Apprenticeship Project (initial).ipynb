{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4798a73c-1fd0-4d93-8641-066f2d7ec2f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from prophet import Prophet\n",
    "from pyspark.sql.functions import to_date, to_timestamp, col, last, when, lit, dayofweek, dayofmonth, dayofyear, month, weekofyear, current_date, date_sub, datediff, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import ticker\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import product\n",
    "import joblib\n",
    "import holidays\n",
    "\n",
    "df = spark.read.table('data_experience_commercial.cbt_1423_rtsuite.master').select('charge_dt','dtg','chargeproduct','unt_pre','flightkey','flight_dt','rev_pre','channel','unt_net')\n",
    "dmh = spark.read.table('data_experience_commercial.cbt_0923_segmentfinder.dimensions_history').select('flightkey','onsale_dt','ty_capacity','routetype','region','route')\n",
    "df = df.join(dmh, on='flightkey', how='left')\n",
    "df = df[(df['chargeproduct']=='Ticket') & (df['dtg'] >= 0) & (df['region'].isin(['UK-London','UK-Regions'])) & (df['routetype'] == 'Domestic')]\n",
    "df = df.filter((col('dtg') < (datediff(col('flight_dt'), col('onsale_dt')) - 25)) & (col('charge_dt') >= '2019-01-01'))\n",
    "window_spec = Window.partitionBy('flightkey').orderBy(col('dtg').desc())\n",
    "df = df.withColumn('pax_net', spark_sum('unt_net').over(window_spec))\n",
    "df = df.withColumn('load_factor', col('pax_net')/col('ty_capacity'))\n",
    "df = df.withColumn('unt_pre', F.when(col('unt_pre') < 0, 0).otherwise(col('unt_pre')))\n",
    "df = df.withColumn('rev_pre', F.when(col('rev_pre') < 0, 0).otherwise(col('rev_pre')))\n",
    "df = df[df['dtg'] <= 364]\n",
    "#df = df.withColumn('channel_index', when(col('channel') == 'Web/App', 1).otherwise(0))\n",
    "df = df.groupby('route','charge_dt','dtg').agg(F.sum('unt_pre').alias('unt_pre'), F.sum('rev_pre').alias('rev_pre'), F.sum('ty_capacity').alias('ty_capacity'), F.avg('load_factor').alias('load_factor')).orderBy('charge_dt','dtg')\n",
    "df = df.toPandas()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def optimize_df(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optimizes DataFrame dtypes using a primary rule for datetime columns\n",
    "    and then general rules for other dtypes.\n",
    "\n",
    "    :param df: pandas DataFrame to optimize.\n",
    "    :param verbose: If True, prints out the memory reduction report.\n",
    "    :return: Optimized pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"Initial memory usage: {start_mem:.2f} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Rule 1 (Highest Priority): Convert columns with 'dt' in their name to datetime.\n",
    "        if '_dt' in col.lower():\n",
    "            # Using errors='coerce' will turn un-convertible values into NaT (Not a Time)\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            # Once done, skip to the next column\n",
    "            continue\n",
    "\n",
    "        # Get the column's type *after* the potential datetime conversion\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Rule 2: Skip columns that are already categorical\n",
    "        if pd.api.types.is_categorical_dtype(col_type):\n",
    "            continue\n",
    "\n",
    "        # Rule 3: Handle object columns (that are NOT datetime)\n",
    "        if col_type == 'object':\n",
    "            try:\n",
    "                # Try to make them numbers (int)\n",
    "                df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            except (ValueError, TypeError):\n",
    "                # If that fails, make them categories if it's efficient\n",
    "                if df[col].nunique() / len(df[col]) < 0.5:\n",
    "                    df[col] = df[col].astype('category')\n",
    "        \n",
    "        # Rule 4: Downcast existing numeric columns\n",
    "        elif pd.api.types.is_numeric_dtype(col_type):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    if verbose:\n",
    "        end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        reduction = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Final memory usage: {end_mem:.2f} MB ({reduction:.2f}% reduction)\")\n",
    "\n",
    "    return df\n",
    "optimize_df(df)\n",
    "#Memory Optimisation\n",
    "#df['route'] = df['route'].astype('category')\n",
    "#df['dtg'] = pd.to_numeric(df['dtg'], downcast='integer')\n",
    "#df['unt_pre'] = pd.to_numeric(df['unt_pre'], downcast='integer')\n",
    "#df['ty_capacity'] = pd.to_numeric(df['ty_capacity'], downcast='integer')\n",
    "#df['rev_pre'] = pd.to_numeric(df['rev_pre'], downcast='float')\n",
    "#df['load_factor'] = pd.to_numeric(df['load_factor'], downcast='float')\n",
    "#df['charge_dt'] = pd.to_datetime(df['charge_dt'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff91655-25f8-425c-bbcf-5a45d7401332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "group = df.copy()\n",
    "group = group[group['dtg'].isin([0,7,14,21,28,56,84,112,140,168,196,224,252])]\n",
    "group = group[group['charge_dt'] >= '2022-01-01']\n",
    "\n",
    "#filtering to routes active in the last 12 months\n",
    "group['year_month'] = group['charge_dt'].dt.to_period('M')\n",
    "latest_month = group['year_month'].max()\n",
    "last_12_months = pd.period_range(end=latest_month, periods=12, freq='M')\n",
    "group_recent = group[group['year_month'].isin(last_12_months)]\n",
    "route_month_counts = (group_recent.groupby('route')['year_month'].nunique())\n",
    "active_routes = route_month_counts[route_month_counts == 12].index\n",
    "group = group[group['route'].isin(active_routes)]\n",
    "\n",
    "#Plotting the booking profiles\n",
    "booking_profiles = (group.groupby(['route','dtg'])['load_factor'].mean().unstack(fill_value=0))\n",
    "booking_profiles.info()\n",
    "#X = StandardScaler().fit_transform(booking_profiles)\n",
    "#all_distances = pairwise_distances(X, metric='euclidean')\n",
    "#neig_distances = [np.min(row[np.nonzero(row)]) for row in all_distances]\n",
    "#plt.hist(neig_distances, bins=2000)\n",
    "#plt.xlabel('Minimum Distance')\n",
    "#plt.ylabel('Frequency')\n",
    "#plt.title('Histogram of Minimum Distances')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d25fcae-55e9-4f00-ba22-cec621ba17e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "group = df.copy()\n",
    "group['load_factor'] = group['unt_pre']/group['ty_capacity']\n",
    "group['charge_dt'] = pd.to_datetime(group['charge_dt'])\n",
    "group = group[group['charge_dt'] >= '2022-01-01']\n",
    "group['year_month'] = group['charge_dt'].dt.to_period('M')\n",
    "latest_month = group['year_month'].max()\n",
    "last_12_months = pd.period_range(end=latest_month, periods=12, freq='M')\n",
    "group_recent = group[group['year_month'].isin(last_12_months)]\n",
    "route_month_counts = (group_recent.groupby('route')['year_month'].nunique())\n",
    "active_routes = route_month_counts[route_month_counts == 12].index\n",
    "group = group[group['route'].isin(active_routes)]\n",
    "booking_profiles = (group.groupby(['route','dtg'])['load_factor'].mean().unstack(fill_value=0))\n",
    "X = StandardScaler().fit_transform(booking_profiles)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "booking_profiles['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48b2e77-43df-4773-95c3-faea7d6ab04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "route_clusters = booking_profiles['cluster'].reset_index()\n",
    "cluster_groups = route_clusters.groupby('cluster')['route'].apply(list)\n",
    "\n",
    "# Step 3: Convert to a DataFrame with clusters as columns\n",
    "from itertools import zip_longest\n",
    "import pandas as pd\n",
    "\n",
    "# Transpose the grouped list into columns\n",
    "cluster_df = pd.DataFrame(\n",
    "    zip_longest(*cluster_groups.values),  # Unpack each list into columns\n",
    "    columns=[f'Cluster {i}' for i in cluster_groups.index]\n",
    ")\n",
    "\n",
    "# Step 4: View the table\n",
    "print(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6154f166-2317-484c-bfca-e31d2ade84d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "booking_profiles_reset = booking_profiles.reset_index().melt(id_vars=['route', 'cluster'], var_name='dtg', value_name='mean_load_factor')\n",
    "booking_profiles_reset = booking_profiles_reset[booking_profiles_reset['dtg'] != 'cluster']\n",
    "booking_profiles_reset['dtg'] = booking_profiles_reset['dtg'].astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=booking_profiles_reset, x='dtg', y='mean_load_factor', hue='cluster', estimator='mean', ci=None)\n",
    "plt.title('Average Booking Curve per Cluster')\n",
    "plt.xlabel('Days to Go (DTG)')\n",
    "plt.ylabel('Mean Load Factor')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6efd20c-e4c2-496d-94bb-553173d89235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=3)  \n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to booking_profiles\n",
    "booking_profiles['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a94e41cf-298a-4813-bdd5-b09123a2013c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "booking_profiles_reset = booking_profiles.reset_index().melt(id_vars=['route', 'cluster'], var_name='dtg', value_name='mean_load_factor')\n",
    "booking_profiles_reset = booking_profiles_reset[booking_profiles_reset['dtg'] != 'cluster']\n",
    "booking_profiles_reset['dtg'] = booking_profiles_reset['dtg'].astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=booking_profiles_reset, x='dtg', y='mean_load_factor', hue='cluster', estimator='mean', ci=None)\n",
    "plt.title('Average Booking Curve per Cluster')\n",
    "plt.xlabel('Days to Go (DTG)')\n",
    "plt.ylabel('Mean Load Factor')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0be5e7f-d9eb-4763-bdf0-45ba2e2919f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import pandas as pd\n",
    "\n",
    "route_clusters = booking_profiles.reset_index()[['route', 'cluster']]\n",
    "cluster_groups = route_clusters.groupby('cluster')['route'].apply(list)\n",
    "\n",
    "cluster_df = pd.DataFrame(\n",
    "    zip_longest(*cluster_groups.values),\n",
    "    columns=[f'Cluster {i}' for i in cluster_groups.index]\n",
    ")\n",
    "\n",
    "print(cluster_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a7dc93-5606-433e-93fe-a0628e41ff4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lag_days = [7, 14, 21, 28, 364]\n",
    "cols_to_lag = ['unt_pre', 'load_factor']\n",
    "\n",
    "def generate_lag_features(df, value_columns, lag_days, date_col='charge_dt', dtg_col='dtg'):\n",
    "    \"\"\"\n",
    "    Generate lag features for multiple value columns at specified lag days.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas.DataFrame\n",
    "    - value_columns: list of column names to generate lag features for\n",
    "    - lag_days: list of integers, each representing a lag in days\n",
    "    - date_col: name of the charge date column\n",
    "    - dtg_col: name of the dtg column (to group by)\n",
    "    \n",
    "    Returns:\n",
    "    - df with added lag features\n",
    "    \"\"\"\n",
    "    df = df.sort_values([dtg_col, date_col])\n",
    "    df = df.set_index([date_col, dtg_col])\n",
    "\n",
    "    for col in value_columns:\n",
    "        for lag in lag_days:\n",
    "            lag_col_name = f'{col}_lag{lag}'\n",
    "            df[lag_col_name] = df.groupby(level=1)[col].shift(lag)\n",
    "\n",
    "    return df.reset_index()\n",
    "\n",
    "df = generate_lag_features(df, cols_to_lag, lag_days)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f23e830-a3b4-4d1d-93f6-f2814f9fa71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "df['rev_pre'] = df['rev_pre'].round(0).astype(int)\n",
    "df['yield'] = df['rev_pre'] / df['unt_pre']\n",
    "df['yield'] = df['yield'].fillna(0) \n",
    "df['yield'] = df['yield'].round(0).astype(int)\n",
    "df.drop(['rev_pre'], axis=1, inplace=True)\n",
    "df.drop(['load_factor'], axis=1, inplace=True)\n",
    "\n",
    "def create_features(df):\n",
    "    df['flight_dt'] = pd.to_datetime(df['charge_dt'] + pd.to_timedelta(df['dtg'], unit='D'))\n",
    "    df['charge_dt'] = pd.to_datetime(df['charge_dt'])\n",
    "\n",
    "    df['flight_month'] = df['flight_dt'].dt.month.astype(int)\n",
    "    df['flight_dow'] = df['flight_dt'].dt.dayofweek.astype(int)\n",
    "    df['flight_dom'] = df['flight_dt'].dt.day.astype(int)\n",
    "    df['flight_doy'] = df['flight_dt'].dt.dayofyear.astype(int)\n",
    "    df['flight_year'] = df['flight_dt'].dt.year.astype(int)\n",
    "\n",
    "    df['charge_month'] = df['charge_dt'].dt.month.astype(int)\n",
    "    df['charge_dow'] = df['charge_dt'].dt.dayofweek.astype(int)\n",
    "    df['charge_dom'] = df['charge_dt'].dt.day.astype(int)\n",
    "    df['charge_doy'] = df['charge_dt'].dt.dayofyear.astype(int)\n",
    "    df['charge_year'] = df['charge_dt'].dt.year.astype(int)\n",
    "\n",
    "    #df.set_index('charge_dt', inplace=True)\n",
    "    #df = df.sort_index()\n",
    "\n",
    "create_features(df)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae9fb4a2-bf70-4952-b07d-9e9d5ac4fdcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc7276-f672-42b5-bb03-0280e6becff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "uk_holidays = holidays.UK(years=range(2022, 2026))\n",
    "holidays_df = pd.DataFrame([(date, name) for date, name in uk_holidays.items()], columns=['ds', 'holiday'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81336b4-d197-41cd-9e5b-aca123500382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "additional_holidays = pd.DataFrame([\n",
    "    {'ds': '2022-04-18', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2022-08-29', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2023-04-10', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2023-08-28', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2024-04-01', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2024-08-26', 'holiday': 'Summer Bank Holiday'},\n",
    "    {'ds': '2025-04-21', 'holiday': 'Easter Monday'},\n",
    "    {'ds': '2025-08-25', 'holiday': 'Summer Bank Holiday'},\n",
    "])\n",
    "holidays_df = pd.concat([holidays_df, additional_holidays], ignore_index=True)\n",
    "holidays_df.drop_duplicates(inplace=True)\n",
    "holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
    "holidays_df.sort_values(by='ds', inplace=True)\n",
    "holidays_df.reset_index(drop=True, inplace=True)\n",
    "holidays_df\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1be869-8f30-48c2-ab6a-0e2015be9d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holidays_df[\"ds\"] = pd.to_datetime(holidays_df[\"ds\"])\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'charge_dt', 'holiday': 'charge_dt_holiday'}), how='left', on='charge_dt')\n",
    "df = df.merge(holidays_df.rename(columns={'ds': 'flight_dt', 'holiday': 'flight_dt_holiday'}), how='left', on='flight_dt')\n",
    "df['is_charge_date_holiday'] = df['charge_dt_holiday'].notnull().astype(int)\n",
    "df['is_flight_date_holiday'] = df['flight_dt_holiday'].notnull().astype(int)\n",
    "df.drop(['charge_dt_holiday', 'flight_dt_holiday'], axis=1, inplace=True)\n",
    "df = df.sort_values(by=['charge_dt', 'dtg'], ascending=[True, True])\n",
    "df.set_index('charge_dt', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ad0dd8-948d-46fe-b277-50eea50088f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130e08a0-96ea-4a54-bc26-4ef5a021296b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_sales = df.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales.plot(style='-', figsize=(20,5), title = 'sales by charge date', y='unt_pre', x='charge_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a916c1-99ba-4b28-a259-577cd2c6f4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''df = df[df.index >= '2022-01-01']\n",
    "total_sales = df.groupby('charge_dt')[['unt_pre','promoseats']].sum()\n",
    "total_sales['promoseats'] = (total_sales['promoseats'] > 0).astype(int)\n",
    "total_sales['promo_change'] = total_sales['promoseats'].ne(total_sales['promoseats'].shift()).cumsum()\n",
    "promo_segments = total_sales.groupby('promo_change')\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.plot(total_sales.index, total_sales['unt_pre'], label='Sales')\n",
    "plt.title('Sales by Charge Date')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Shade background where promoseats == 1\n",
    "label_added = False\n",
    "for _, segment in promo_segments:\n",
    "    if segment['promoseats'].iloc[0] == 1:\n",
    "        start = segment.index[0]\n",
    "        end = segment.index[-1]\n",
    "        ax.axvspan(start, end, color='lightblue', alpha=1,  label='Promotion Period' if not label_added else None)\n",
    "        label_added = True\n",
    "\n",
    "ax.legend()\n",
    "plt.show()'''\n",
    "\n",
    "df = df[df.index >= '2022-01-01']\n",
    "total_sales_by_charge_dt = df.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales_by_charge_dt.plot(style='-', figsize=(20,5), title = 'sales by charge date', y='unt_pre', x='charge_dt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04dc715-59c0-41b4-9233-7551663bd86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['flight_dt'], axis=1, inplace=True)\n",
    "total_sales_by_dtg = df.groupby('dtg')['unt_pre'].sum().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "total_sales_by_dtg.plot(style='-', figsize=(20,5), title = 'sales by dtg since 2022', y='unt_pre', x='dtg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25deb798-ce28-4204-a9d8-59d04c270f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_reset = df.reset_index()\n",
    "df_reset['charge_dt'] = df_reset['charge_dt'].astype(str)\n",
    "df_reset = df_reset.groupby(['dtg', 'charge_dt'])['unt_pre'].sum().reset_index()\n",
    "fig = px.line(df_reset, \n",
    "                 x='dtg', \n",
    "                 y='unt_pre', \n",
    "                 animation_frame='charge_dt',  # Add dtg as an animation frame\n",
    "                 title='Sales by DTG with charge date variations')\n",
    "\n",
    "fig.update_xaxes(range=[0, 200])\n",
    "fig.update_yaxes(range=[0, 800])\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Days To Go',\n",
    "    yaxis_title='Sales',\n",
    "    legend_title='Charge Date',\n",
    "    height=900,\n",
    "    width=1400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a1bbd2-b12e-4540-87b0-98e548329f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df is the original DataFrame\n",
    "df_melt = df.melt(id_vars='unt_pre', value_vars=['flight_month', 'charge_month'], var_name='month_type', value_name='month')\n",
    "\n",
    "# Group by month and month type, and calculate mean sales\n",
    "mean_sales = df_melt.groupby(['month', 'month_type'])['unt_pre'].mean().reset_index()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(data=mean_sales, x='month', y='unt_pre', hue='month_type')\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Mean Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Mean Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a0c9eb6-c222-4346-a611-dd1ed6cdabf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assuming df is the original DataFrame\n",
    "df_melt = df.melt(id_vars='unt_pre', value_vars=['flight_dow', 'charge_dow'], var_name='dow_type', value_name='dow')\n",
    "mean_sales = df_melt.groupby(['dow', 'dow_type'])['unt_pre'].mean().reset_index()\n",
    "#mean_sales['dow'] = pd.Categorical(mean_sales['dow'], categories=[str(i) for i in range(0, 7)], ordered=True)\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=mean_sales, x='dow', y='unt_pre', hue='dow_type')\n",
    "plt.style.use('ggplot')\n",
    "plt.title('Mean Sales by DoW')\n",
    "plt.xlabel('DoW')\n",
    "plt.ylabel('Mean Sales')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b939ae4-ac68-49b0-8ffd-959811383a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by charge day of month and calculate mean sales\n",
    "mean_sales = df.groupby('charge_dom')['unt_pre'].mean().reset_index()\n",
    "mean_yield = df.groupby('charge_dom')['yield'].mean().reset_index()\n",
    "\n",
    "#mean_sales['charge_dom'] = pd.to_numeric(mean_sales['charge_dom'])\n",
    "#mean_yield['charge_dom'] = pd.to_numeric(mean_yield['charge_dom'])\n",
    "#mean_sales = mean_sales.sort_values(by='charge_dom')\n",
    "#mean_yield = mean_yield.sort_values(by='charge_dom')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 8))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Plot mean sales\n",
    "ax1.plot(mean_sales['charge_dom'], mean_sales['unt_pre'], marker='o', linewidth=12, color='tab:red', label='Mean Sales')\n",
    "ax1.set_xlabel('Charge Day of Month')\n",
    "ax1.set_ylabel('Mean Sales', color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Create second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(mean_yield['charge_dom'], mean_yield['yield'], marker='s', linewidth=12, color='tab:blue', label='Mean Yield')\n",
    "ax2.set_ylabel('Mean Yield', color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Title and legend\n",
    "fig.suptitle('Mean Sales and Yield by Charge Day of Month (2022 - ToDate)')\n",
    "fig.legend(loc='upper right', bbox_to_anchor=(0.9, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "# Plot the line graph\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.style.use('ggplot')\n",
    "plt.plot(mean_sales['charge_dom'], mean_sales['unt_pre'], marker='o', linewidth=15)\n",
    "plt.title('Mean Sales by Charge Day of Month')\n",
    "plt.xlabel('Charge Day of Month')\n",
    "plt.ylabel('Mean Sales')\n",
    "display(plt.show())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1b07e5-a271-4c24-9093-d374268dc7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[(df.index > '2024-01-01') & (df.index < '2024-01-31')].groupby('charge_dt')['unt_pre'].mean().plot(figsize=(20,5), title = 'sales by charge date (Jan24)', y='unt_pre', linewidth=10)\n",
    "plt.style.use('ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f56257-353a-43c0-8d7d-d11c9d3ae24a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.drop(['yield'], axis=1, inplace=True)\n",
    "cyclic_cols=['flight_dom', 'flight_doy', 'charge_dom', 'charge_doy', 'flight_month', 'charge_month', 'flight_dow', 'charge_dow']\n",
    "num_cols=['charge_year','flight_year', 'ty_capacity']\n",
    "\n",
    "def encode_cyclic_features(df, cyclic_cols):\n",
    "    for col in cyclic_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        max_val = df[col].max()\n",
    "        df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "        df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def scale_num_cols(df, num_cols, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "    else:\n",
    "        df[num_cols] = scaler.transform(df[num_cols])\n",
    "    return df, scaler\n",
    "\n",
    "encode_cyclic_features(df, cyclic_cols)\n",
    "df, scaler = scale_num_cols(df, num_cols)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4280a4f-680e-4b19-b3e5-71fcba1a60d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optimize_df(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de1e6f14-d693-4321-b457-d207d140c65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Error Metric\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c0a2e0-6d9c-40c3-afd6-3688882a64db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "split_date = pd.to_datetime(spark.sql(\"SELECT current_date()\").collect()[0][0]) - pd.DateOffset(days=168)\n",
    "train_prophet_plot = df.loc[df.index < split_date].groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "val_prophet_plot = df.loc[df.index >= split_date].groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(train_prophet_plot['charge_dt'],train_prophet_plot['unt_pre'], label='Train', color='blue')\n",
    "plt.plot(val_prophet_plot['charge_dt'], val_prophet_plot['unt_pre'], label='Validation', color='red')\n",
    "plt.axvline(pd.to_datetime(split_date), linestyle='--', color='black', label='Split Date')\n",
    "plt.legend()\n",
    "plt.title('Sales Before and After Split Date')\n",
    "plt.xlabel('Charge Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd31b82-3798-47f2-a101-9b2dc2965103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Prophet\n",
    "df_prophet = df.groupby(['dtg', 'charge_dt'])['unt_pre', 'ty_capacity', 'unt_pre_lag7', 'unt_pre_lag14', 'unt_pre_lag21', 'unt_pre_lag28'].sum().reset_index()\n",
    "train_prophet = df_prophet.loc[df_prophet['charge_dt'] < split_date]\n",
    "val_prophet = df_prophet.loc[df_prophet['charge_dt'] >= split_date]\n",
    "train_df = train_prophet.reset_index()\n",
    "val_df = val_prophet.reset_index()\n",
    "all_forecasts = []\n",
    "\n",
    "for dtg_value in train_df['dtg'].unique():\n",
    "    # Prepare training data for this dtg\n",
    "    train_subset = train_df[train_df['dtg'] == dtg_value][['charge_dt', 'unt_pre', 'ty_capacity', 'unt_pre_lag7', 'unt_pre_lag14', 'unt_pre_lag21', 'unt_pre_lag28']].copy()\n",
    "    train_subset.rename(columns={'charge_dt': 'ds', 'unt_pre': 'y'}, inplace=True)\n",
    "    train_subset.sort_values('ds', inplace=True)\n",
    "\n",
    "    # Prepare validation data (future dates)\n",
    "    val_subset = val_df[val_df['dtg'] == dtg_value][['charge_dt', 'unt_pre', 'ty_capacity', 'unt_pre_lag7', 'unt_pre_lag14', 'unt_pre_lag21', 'unt_pre_lag28']].copy()\n",
    "    val_subset.rename(columns={'charge_dt': 'ds'}, inplace=True)\n",
    "    val_subset.sort_values('ds', inplace=True)\n",
    "\n",
    "    # Skip if not enough data\n",
    "    if len(train_subset) < 10 or val_subset.empty:\n",
    "        continue\n",
    "\n",
    "    # Train Prophet\n",
    "    model = Prophet(holidays=holidays_df)\n",
    "    regressors = ['ty_capacity', 'unt_pre_lag7', 'unt_pre_lag14', 'unt_pre_lag21', 'unt_pre_lag28']\n",
    "    for reg in regressors:\n",
    "        model.add_regressor(reg)\n",
    "    model.fit(train_subset)\n",
    "\n",
    "    # Forecast only for validation dates\n",
    "    forecast = model.predict(val_subset)\n",
    "\n",
    "    # Add metadata\n",
    "    forecast['dtg'] = dtg_value\n",
    "    actual_values = val_df[val_df['dtg'] == dtg_value].set_index('charge_dt').reindex(forecast['ds'])['unt_pre'].values\n",
    "    forecast['actual'] = actual_values\n",
    "\n",
    "    all_forecasts.append(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper', 'dtg', 'actual']])\n",
    "\n",
    "prophet_val_forecasts = pd.concat(all_forecasts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e4ccec-7bbd-48fc-8fbc-8bb5307bdfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_forecast = (prophet_val_forecasts.groupby('ds').agg({'yhat': 'sum','yhat_lower': 'sum', 'yhat_upper': 'sum', 'actual': 'sum'}).reset_index())\n",
    "actuals_all = pd.concat([train_df, val_df])\n",
    "actuals_all = actuals_all.rename(columns={'charge_dt': 'ds', 'unt_pre': 'y'})\n",
    "agg_actuals = (actuals_all.groupby('ds').agg({'y': 'sum'}).reset_index())\n",
    "plot_df = pd.merge(agg_forecast, agg_actuals, on='ds', how='outer').sort_values('ds')\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "plot_df_val = plot_df[plot_df['ds'] >= split_date]\n",
    "plot_df_train = plot_df[plot_df['ds'] < split_date]\n",
    "\n",
    "ax.plot(plot_df_val['ds'], plot_df_val['y'], 'k.', alpha=0.6, label='Actual')\n",
    "ax.plot(plot_df_train['ds'], plot_df_train['y'], 'red', linewidth=1.5, label='Train') \n",
    "\n",
    "ax.plot(plot_df['ds'], plot_df['yhat'], color='blue', label='Forecast')\n",
    "ax.fill_between(plot_df['ds'], plot_df['yhat_lower'], plot_df['yhat_upper'], color='skyblue', alpha=0.3, label='Confidence Interval')\n",
    "\n",
    "ax.axvline(x=split_date, color='black', linestyle='--', lw=2, label='Forecast Start')\n",
    "ax.set_title('Forecasted Demand with Prophet (Aggregated)', fontsize=18, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=14)\n",
    "ax.set_ylabel('Sales', fontsize=14)\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baa0d31-52a2-4fe4-8f2a-17af29eb5235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(agg_forecast['ds'], agg_forecast['actual'], label='Actual', marker='o')\n",
    "plt.plot(agg_forecast['ds'], agg_forecast['yhat'], label='Forecast (Prophet)', marker='x')\n",
    "plt.xlabel('Charge Date')\n",
    "plt.ylabel('Total Sales Across All DTGs')\n",
    "plt.title('Prophet Forecast vs Actual – Aggregated Over DTG')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92617e3e-b48e-4fc7-90a5-0fc380aa652f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mse_prophet = np.sqrt(mean_squared_error(agg_forecast['actual'], agg_forecast['yhat']))\n",
    "mape_prophet = mean_absolute_percentage_error(agg_forecast['actual'], agg_forecast['yhat'])\n",
    "print(f'MSE: {mse_prophet:.2f}')\n",
    "print(f'MAPE: {mape_prophet:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01fe76d8-c4d3-49f7-80a7-c4ae1c162fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''%python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.005, 0.01, 0.1],\n",
    "    #'subsample': [0.8, 1.0],\n",
    "    #'colsample_bytree': [0.8, 1.0],\n",
    "    #'lambda': [0.1, 1.0],\n",
    "    #'alpha': [0, 0.1],\n",
    "}\n",
    "\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "outer_cv = TimeSeriesSplit(n_splits=5)\n",
    "inner_cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "outer_scores = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(df):\n",
    "    X_train_outer = df.iloc[train_idx].drop('unt_net', axis=1).copy()\n",
    "    y_train_outer = df.iloc[train_idx]['unt_net'].copy()\n",
    "    X_test_outer = df.iloc[test_idx].drop('unt_net', axis=1).copy()\n",
    "    y_test_outer = df.iloc[test_idx]['unt_net'].copy()\n",
    "\n",
    "    X_train_processed = preprocessor.fit_preprocess(X_train_outer)\n",
    "    X_test_processed = preprocessor.transform_preprocess(X_test_outer)\n",
    "\n",
    "    model = XGBRegressor(objective='reg:pseudohubererror', base_score=0.5, boosting='gbtree', early_stopping_rounds=50, max_depth=3, n_estimators=500, learning_rate=0.01)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=inner_cv, n_jobs=-1, verbose=1, scoring=rmse_scorer)\n",
    "\n",
    "    grid_search.fit(X_train_processed, y_train_outer, eval_set=[(X_test_processed, y_test_outer)])\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test_processed, y_test_outer)\n",
    "    outer_scores.append(test_score)\n",
    "\n",
    "    print(f'Outer fold score: {test_score}, best params: {grid_search.best_params_}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823b993d-9273-4a59-a826-2d6f9ae28cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "\n",
    "unique_dates = df.index.sort_values().unique()\n",
    "tscv = TimeSeriesSplit(n_splits=5, test_size=168)\n",
    "\n",
    "fig, axs = plt.subplots(5, 1, figsize=(20, 10), sharex=True)\n",
    "plt.style.use('ggplot')\n",
    "fold=0\n",
    "preds = []\n",
    "scores = []\n",
    "# Nested cross-validation\n",
    "for train_index, val_index in tscv.split(unique_dates):\n",
    "\n",
    "    train_dates = unique_dates[train_index]\n",
    "    val_dates = unique_dates[val_index]\n",
    "\n",
    "    train_data = df.loc[train_dates]\n",
    "    val_data = df.loc[val_dates]\n",
    "\n",
    "    total_sales_train = train_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_val = val_data.groupby('charge_dt')['unt_pre'].sum().reset_index()\n",
    "    total_sales_train.plot(ax=axs[fold], label='Train', x='charge_dt', y='unt_pre', style='-')\n",
    "    total_sales_val.plot(ax=axs[fold], label='Val', x='charge_dt', y='unt_pre', style='-')\n",
    "    axs[fold].axvline(val_data.index.min(), linestyle='--', color='black')\n",
    "    axs[fold].set_title(f'Fold {fold+1}')\n",
    "    fold += 1\n",
    "\n",
    "    X_train = train_data.drop('unt_pre', axis=1)\n",
    "    y_train = train_data['unt_pre']\n",
    "\n",
    "    X_test = val_data.drop('unt_pre', axis=1)\n",
    "    y_test = val_data['unt_pre']\n",
    "\n",
    "    model = xgb.XGBRegressor(base_score=0.5, booster ='gbtree', n_estimators=1000, early_stopping_rounds=50, max_depth=3, learning_rate=0.01, objective='reg:pseudohubererror', enable_categorical=True)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=100)\n",
    "\n",
    "    y_pred=model.predict(X_test)\n",
    "    preds.extend(y_pred)\n",
    "    score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f14fc5-e055-4075-a260-5624280535b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'individual scores: {scores}')\n",
    "print(f'combined score: {np.mean(scores)}')\n",
    "print(f'std: {np.std(scores)}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b8a62d0-7c37-4379-bbb8-71ac38e5739d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({'date': X_test.index, 'prediction': y_pred, 'dtg': X_test['dtg'], 'route': X_test['route']})\n",
    "actual = pd.DataFrame({'date': X_test.index, 'unt_pre': y_test, 'dtg': X_test['dtg'], 'route': X_test['route']})\n",
    "y_pred_total = predictions.groupby('date')['prediction'].sum().reset_index()\n",
    "y_test_total = actual.groupby('date')['unt_pre'].sum().reset_index()\n",
    "y_pred_total.set_index('date', inplace=True)\n",
    "y_test_total.set_index('date', inplace=True)\n",
    "mape_xgb = mean_absolute_percentage_error(y_test_total, y_pred_total)\n",
    "mse_xgb = np.sqrt(mean_squared_error(y_test_total, y_pred_total))\n",
    "print(f'MSE: {mse_xgb:.2f}')\n",
    "print(f'MAPE: {mape_xgb:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64b7be3-1c45-475d-91e1-896975658aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_total.index, y_test_total['unt_pre'], label='Actual', marker='o')\n",
    "plt.plot(y_pred_total.index, y_pred_total['prediction'], label='Forecast', marker='x')\n",
    "plt.xlabel('Charge Date')\n",
    "plt.ylabel('Total Sales Across All DTGs')\n",
    "plt.title('XGB Forecast vs Actual – Aggregated Over DTG')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e185a249-f34b-4472-be97-cf34e8b7d1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "optimize_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba5d180-f8c1-4214-9cf4-87a9c58e6c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Hierarchical Reconciliation\n",
    "df_hierarchical = df.rename(columns={'charge_dt': 'ds', 'unt_pre': 'y'})\n",
    "df_hierarchical['unique_id'] = df_hierarchical['route'].astype(str) + '_' + df_hierarchical['dtg'].astype(str)\n",
    "df_hierarchical = df_hierarchical[['ds', 'unique_id', 'route', 'y', 'dtg']]\n",
    "df_hierarchical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50adf141-4d56-41b0-a3e8-4b8d60e198ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# This wrapper class is the key to using any scikit-learn-like model\n",
    "class XGBoostRegressorWithFeatures(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = xgb.XGBRegressor(**kwargs)\n",
    "\n",
    "    lag_days = [7, 14, 21, 28, 364]\n",
    "    cols_to_lag = ['unt_pre', 'load_factor']\n",
    "\n",
    "    def generate_lag_features(df, value_columns, lag_days, date_col='charge_dt', dtg_col='dtg'):\n",
    "        \n",
    "        df = df.sort_values([dtg_col, date_col])\n",
    "        df = df.set_index([date_col, dtg_col])\n",
    "\n",
    "        for col in value_columns:\n",
    "            for lag in lag_days:\n",
    "                lag_col_name = f'{col}_lag{lag}'\n",
    "                df[lag_col_name] = df.groupby(level=1)[col].shift(lag)\n",
    "\n",
    "        return df.reset_index()\n",
    "\n",
    "\n",
    "    def _create_features(self, df):\n",
    "        \"\"\"Creates time-series features from a dataframe.\"\"\"\n",
    "        X = df.copy()\n",
    "        # --- IMPORTANT: Add all the same feature engineering you used before ---\n",
    "        X['flight_dt'] = pd.to_datetime(X['ds'] + pd.to_timedelta(X['dtg'], unit='D'))\n",
    "\n",
    "        X['flight_month'] = X['flight_dt'].dt.month.astype(int)\n",
    "        X['flight_dow'] = X['flight_dt'].dt.dayofweek.astype(int)\n",
    "        X['flight_dom'] = X['flight_dt'].dt.day.astype(int)\n",
    "        X['flight_doy'] = X['flight_dt'].dt.dayofyear.astype(int)\n",
    "        X['flight_year'] = X['flight_dt'].dt.year.astype(int)\n",
    "\n",
    "        X['charge_month'] = X['ds'].dt.month.astype(int)\n",
    "        X['charge_dow'] = X['ds'].dt.dayofweek.astype(int)\n",
    "        X['charge_dom'] = X['ds'].dt.day.astype(int)\n",
    "        X['charge_doy'] = X['ds'].dt.dayofyear.astype(int)\n",
    "        X['charge_year'] = X['ds'].dt.year.astype(int)\n",
    "\n",
    "        lag_days = [7, 14, 21, 28, 364]\n",
    "        cols_to_lag = ['unt_pre', 'load_factor']\n",
    "        generate_lag_features(X)\n",
    "\n",
    "        # ... add your lag features, etc.\n",
    "        # This part must match the features your model was trained on.\n",
    "        return X.drop(columns=['ds'])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_featured = self._create_features(X)\n",
    "        self.model.fit(X_featured, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_featured = self._create_features(X)\n",
    "        return self.model.predict(X_featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dddda85-08cb-46cc-bf08-479c530558f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class XGBoostPipelineWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **xgb_kwargs):\n",
    "        self.model = xgb.XGBRegressor(**xgb_kwargs)\n",
    "        self.scaler = StandardScaler()\n",
    "        # Store other objects needed for prediction\n",
    "        self.trained_features = []\n",
    "        self.cyclic_cols = ['flight_month', 'flight_dow', 'flight_dom', 'flight_doy', 'charge_month', 'charge_dow', 'charge_dom', 'charge_doy']\n",
    "        self.num_cols_to_scale = ['ty_capacity'] # Add other numeric cols here\n",
    "        self.lag_cols = ['unt_pre_lag7', 'unt_pre_lag14', 'unt_pre_lag21', 'unt_pre_lag28']\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # 1. Combine X and y to create features that depend on the target (like lags)\n",
    "        df_train = X.copy()\n",
    "        df_train['y'] = y\n",
    "\n",
    "        # 2. Run all your feature engineering functions\n",
    "        df_featured = create_features(df_train)\n",
    "        # Note: Lag generation might need adjustment or be pre-computed\n",
    "        # as it depends on historical y values not present in X.\n",
    "        # For simplicity, we assume lags are already present or generated.\n",
    "        \n",
    "        df_featured = encode_cyclic_features(df_featured, self.cyclic_cols)\n",
    "        \n",
    "        # 3. Fit the scaler ON THE TRAINING DATA ONLY\n",
    "        self.scaler.fit(df_featured[self.num_cols_to_scale])\n",
    "        df_featured[self.num_cols_to_scale] = self.scaler.transform(df_featured[self.num_cols_to_scale])\n",
    "        \n",
    "        # 4. Prepare final feature set for XGBoost\n",
    "        # Make sure to drop columns that are not features\n",
    "        y_final = df_featured['y']\n",
    "        X_final = df_featured.drop(columns=['y', 'ds', 'flight_dt']) # drop identifiers/dates\n",
    "        \n",
    "        self.trained_features = X_final.columns\n",
    "        \n",
    "        # 5. Train the model\n",
    "        self.model.fit(X_final, y_final)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # 1. Run the same feature engineering on the new data (X)\n",
    "        df_future = X.copy()\n",
    "        df_featured = create_features(df_future)\n",
    "        df_featured = encode_cyclic_features(df_featured, self.cyclic_cols)\n",
    "        \n",
    "        # 2. Apply the ALREADY FITTED scaler\n",
    "        df_featured[self.num_cols_to_scale] = self.scaler.transform(df_featured[self.num_cols_to_scale])\n",
    "        \n",
    "        # 3. Ensure columns match the training set\n",
    "        X_final = df_featured[self.trained_features]\n",
    "        \n",
    "        # 4. Make predictions\n",
    "        predictions = self.model.predict(X_final)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179b85a0-70ea-47e1-9b6e-2b1b27f1b0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(data=model.feature_importances_, index=model.feature_names_in_, columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33a7cdf-2ec1-4dee-b98e-2682e49a0a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_reset_actual = actual.reset_index()\n",
    "df_reset_actual['date'] = df_reset_actual['date'].astype(str)\n",
    "\n",
    "df_reset_pred = predictions.reset_index()\n",
    "df_reset_pred['date'] = df_reset_pred['date'].astype(str)\n",
    "\n",
    "df_plot_actual = df_reset_actual[df_reset_actual['unt_pre'].notnull()].copy()\n",
    "df_plot_pred = df_reset_pred[df_reset_pred['prediction'].notnull()].copy()\n",
    "\n",
    "df_actual = df_plot_actual[['date','dtg','unt_pre']].copy()\n",
    "df_actual['sales_type'] = 'actual'\n",
    "df_actual['sales_value'] = df_actual['unt_pre']\n",
    "\n",
    "df_pred = df_plot_pred[['date','dtg','prediction']].copy()\n",
    "df_pred['sales_type'] = 'predicted'\n",
    "df_pred['sales_value'] = df_pred['prediction']\n",
    "\n",
    "df_melted = pd.concat([df_actual, df_pred]).sort_values(['date','dtg','sales_type'])\n",
    "\n",
    "min_dtg = 0\n",
    "max_dtg = 200\n",
    "min_sales = 0\n",
    "max_sales = 800\n",
    "\n",
    "fig = px.line(df_melted, x='dtg', y='sales_value', color='sales_type', animation_frame='date', title='Actual vs Predicted by DTG across Charge Dates', color_discrete_map={'unt_net':'red', 'prediction':'blue'})\n",
    "fig.update_xaxes(range=[min_dtg, max_dtg], title='DTG')\n",
    "fig.update_yaxes(range=[min_sales, max_sales], title='Sales')\n",
    "fig.update_layout(legend_title='sales type', height=900, width=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7acc3-1a29-43b0-b3ba-06a2b89ca7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val_data['error'] = np.abs(y_test - y_pred)\n",
    "val_data.groupby(val_data.index)['error'].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0a8311-941c-484a-aa84-e963947bd02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "val_data.groupby(val_data.index)['error'].mean().plot(figsize=(20,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7085c6dc-140e-488f-82ad-572be4333900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "future = pd.date_range(start=df.index.max() + pd.DateOffset(days=1), end=(df.index.max() + pd.DateOffset(days=1)), freq='1d')\n",
    "dtg_values = list(range(0, 301))\n",
    "future_combinations = list(product(future, dtg_values))\n",
    "future_df = pd.DataFrame(future_combinations, columns=['charge_dt', 'dtg'])\n",
    "create_features(future_df)\n",
    "encode_cyclic_features(future_df, cyclic_cols)\n",
    "future_df, _ = scale_num_cols(future_df, num_cols, scaler = joblib.load('scaler.pkl'))\n",
    "for lag in lag_periods:\n",
    "    future_df[f'lag{lag}'] = future_df.apply(lambda row: target_map.get((row.name - pd.Timedelta(f'{lag} day'), row['dtg']), None), axis=1)\n",
    "future_df['isFuture'] = True\n",
    "df['isFuture'] = False\n",
    "df_and_future = pd.concat([df, future_df])\n",
    "df_and_future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac8a2d3-f696-4415-80d2-e41d4952a21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "future_w_features = df_and_future.query('isFuture').copy()\n",
    "future_w_features = future_w_features.drop(columns=['unt_pre', 'isFuture'])\n",
    "future_w_features['pred'] = model.predict(future_w_features)\n",
    "future_w_features.plot(figsize=(20,5), x='dtg', y='pred')\n",
    "plt.title('Predicted Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apprenticeship Project (initial)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
